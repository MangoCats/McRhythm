# TC-I-008-01: Hash-Based Duplicate Detection - Full Workflow

**Test ID:** TC-I-008-01
**Test Type:** Integration Test
**Requirement:** REQ-SPEC032-008 (Hash-Based Duplicate Detection)
**Components:** `hash_deduplicator.rs` + `db/files.rs` (database interaction)
**Priority:** P0 (Critical)
**Estimated Effort:** 1 hour (write + implement)

---

## Test Objective

Verify end-to-end hash-based duplicate detection workflow: hash computation → database storage → duplicate query → bidirectional linking → status marking.

---

## Test Specification

### Setup (Test Environment)

**Database:** SQLite in-memory or temporary file (real database, not mock)

**Initial State:**
- files table empty

**Test Files:**
- `original.mp3` - 1MB audio file, BLAKE3 hash: `abc123...`
- `duplicate.mp3` - Identical content to original.mp3, different filename

### Given (Initial Conditions)

**Step 1:** Import `original.mp3`
- File processed through FILENAME MATCHING (no match)
- fileId assigned: `file-original-uuid`

### When (Action/Input)

**Step 2:** Process `duplicate.mp3` through hash deduplication pipeline

**Actions in Sequence:**
1. Compute hash of `duplicate.mp3` → `abc123...`
2. Store hash in files table under new fileId: `file-duplicate-uuid`
3. Query database for files with matching hash `abc123...`
4. Find match: `file-original-uuid` with status 'INGEST COMPLETE'
5. Update `file-duplicate-uuid` status to 'DUPLICATE HASH'
6. Update matching_hashes bidirectionally:
   - `file-duplicate-uuid`.matching_hashes = `["file-original-uuid"]` (original at front)
   - `file-original-uuid`.matching_hashes = `["file-duplicate-uuid"]`

### Then (Expected Result)

**Database State After Processing:**

**files table:**
| fileId | path | content_hash | status | matching_hashes |
|--------|------|--------------|--------|-----------------|
| file-original-uuid | /Music/original.mp3 | abc123... | INGEST COMPLETE | ["file-duplicate-uuid"] |
| file-duplicate-uuid | /Music/duplicate.mp3 | abc123... | DUPLICATE HASH | ["file-original-uuid"] |

**Processing Outcome:**
- `duplicate.mp3` NOT processed through remaining pipeline (EXTRACTING, SEGMENTING, etc.)
- No passage entries created for duplicate
- No redundant fingerprinting or API calls

### Verify (Specific Assertions)

**Assertion 1:** Hash computed correctly
```rust
let hash = compute_hash("/Music/duplicate.mp3").await?;
assert_eq!(hash, "abc123...");
```

**Assertion 2:** Duplicate file status marked 'DUPLICATE HASH'
```rust
let row = db.query_one("SELECT status FROM files WHERE fileId = ?", [file-duplicate-uuid]).await?;
assert_eq!(row.get::<String>("status"), "DUPLICATE HASH");
```

**Assertion 3:** Bidirectional linking - duplicate references original
```rust
let row = db.query_one("SELECT matching_hashes FROM files WHERE fileId = ?", [file-duplicate-uuid]).await?;
let hashes: Vec<String> = serde_json::from_str(row.get::<String>("matching_hashes"))?;
assert_eq!(hashes.len(), 1);
assert_eq!(hashes[0], "file-original-uuid");
```

**Assertion 4:** Bidirectional linking - original references duplicate
```rust
let row = db.query_one("SELECT matching_hashes FROM files WHERE fileId = ?", [file-original-uuid]).await?;
let hashes: Vec<String> = serde_json::from_str(row.get::<String>("matching_hashes"))?;
assert!(hashes.contains(&"file-duplicate-uuid".to_string()));
```

**Assertion 5:** Original fileId at front of duplicate's matching_hashes list
```rust
assert_eq!(hashes[0], "file-original-uuid"); // Original (INGEST COMPLETE) at index 0
```

**Assertion 6:** No passages created for duplicate
```rust
let count = db.query_one("SELECT COUNT(*) FROM passages WHERE fileId = ?", [file-duplicate-uuid]).await?;
assert_eq!(count.get::<i32>(0), 0);
```

### Pass Criteria

✅ All 6 assertions pass
✅ Bidirectional linking verified in both directions
✅ Duplicate file skipped (no redundant processing)
✅ Database integrity maintained (no orphaned entries)

### Fail Criteria

❌ Hash mismatch (different hash computed for identical content)
❌ Status not set to 'DUPLICATE HASH'
❌ Bidirectional linking incomplete (only one direction updated)
❌ Original fileId not at front of matching_hashes list
❌ Duplicate processed through full pipeline (wasteful)

---

## Test Data

**Original File:**
- Path: `/Music/original.mp3`
- Size: 1MB
- BLAKE3 Hash: `abc123def456...` (64-character hex string)
- Status: 'INGEST COMPLETE' (processed fully)

**Duplicate File:**
- Path: `/Music/duplicate.mp3`
- Size: 1MB
- Content: Byte-for-byte identical to original.mp3
- BLAKE3 Hash: `abc123def456...` (same as original)

**Database Fixture (After Step 1):**
```sql
INSERT INTO files (fileId, path, filename, content_hash, status, matching_hashes)
VALUES ('file-original-uuid', '/Music/original.mp3', 'original.mp3',
        'abc123def456...', 'INGEST COMPLETE', '[]');
```

---

## Implementation Notes

**Hash Algorithm:** BLAKE3 (fast, collision-resistant)
- Library: `blake3` crate
- Input: Entire file content (read in chunks for memory efficiency)
- Output: 64-character hex string

**Database Schema Requirements:**
- files.content_hash: TEXT (indexed for fast lookup)
- files.matching_hashes: JSON (array of fileId strings)

**Update Logic:**
```rust
// 1. Compute hash
let hash = blake3::hash(file_content).to_hex();

// 2. Store hash
db.execute("UPDATE files SET content_hash = ? WHERE fileId = ?", [hash, new_file_id])?;

// 3. Find matches
let matches = db.query("SELECT fileId, status FROM files WHERE content_hash = ? AND fileId != ?", [hash, new_file_id])?;

// 4. If any match has status 'INGEST COMPLETE':
if let Some(completed_match) = matches.iter().find(|m| m.get("status") == "INGEST COMPLETE") {
    // Mark new file as duplicate
    db.execute("UPDATE files SET status = 'DUPLICATE HASH' WHERE fileId = ?", [new_file_id])?;

    // Update bidirectional links
    let completed_id = completed_match.get("fileId");

    // New file references completed file (at front of list)
    db.execute("UPDATE files SET matching_hashes = json_array(?) WHERE fileId = ?", [completed_id, new_file_id])?;

    // Completed file references new file (append to list)
    db.execute("UPDATE files SET matching_hashes = json_insert(matching_hashes, '$[#]', ?) WHERE fileId = ?", [new_file_id, completed_id])?;
}
```

---

## Edge Cases Tested

**Case 1:** Hash collision (extremely unlikely with BLAKE3)
- If collision detected: Log warning, continue processing (do NOT skip)

**Case 2:** Multiple duplicates of same file
- First duplicate links to original
- Second duplicate links to original (not first duplicate)
- All duplicates in original's matching_hashes list

**Case 3:** Duplicate detected but original not yet completed
- Do NOT mark as 'DUPLICATE HASH'
- Continue processing both files
- Update matching_hashes for tracking, but both process fully

---

## Related Tests

- **TC-U-008-01:** Unit test - hash matches 'INGEST COMPLETE' file
- **TC-U-008-02:** Unit test - hash matches 'PENDING' file (continue processing)
- **TC-U-008-04:** Unit test - bidirectional matching_hashes update
- **TC-S-008-01:** System test - import same file twice end-to-end

---

## Traceability

| Item | Reference |
|------|-----------|
| **Requirement** | REQ-SPEC032-008 (Hash-Based Duplicate Detection) |
| **Specification** | [SPEC032_wkmp-ai_refinement_specification.md](../../wip/SPEC032_wkmp-ai_refinement_specification.md) lines 345-351 |
| **Implementation** | wkmp-ai/src/services/hash_deduplicator.rs (to be created), wkmp-ai/src/db/files.rs (to be modified) |
| **Status** | Defined (ready for implementation) |
